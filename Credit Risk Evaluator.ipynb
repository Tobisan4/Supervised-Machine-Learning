{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files to dataframe\n",
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))\n",
    "target_names = [\"High Risk\", \"Low Risk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric and separate target feature for training data\n",
    "# Drop the label to create the X_train data\n",
    "X_train = train_df.drop('target', axis = 1)\n",
    "y_train = train_df['target']\n",
    "\n",
    "# One-hot encoding the X_train data\n",
    "X_train_dummies = pd.get_dummies(X_train)\n",
    "\n",
    "# Converting target labels to 0 and 1\n",
    "y_train_label = LabelEncoder().fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric and separate target feature for testing data\n",
    "# Drop the label to create the X_test data\n",
    "X_test = test_df.drop('target', axis = 1)\n",
    "y_test = test_df['target']\n",
    "\n",
    "# One-hot encoding the X_test data\n",
    "X_test_dummies = pd.get_dummies(X_test)\n",
    "\n",
    "# Converting target labels to 0 and 1\n",
    "y_test_label = LabelEncoder().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing dummy variables to testing set\n",
    "# Find missing columns\n",
    "X_train_header = X_train_dummies.columns\n",
    "X_test_header = X_test_dummies.columns\n",
    "missing_columns = list(set(X_test_header) - set(X_train_header)) + list(set(X_train_header) - set(X_test_header))\n",
    "\n",
    "# Add missing columns and populate with 0s\n",
    "for i in range(len(missing_columns)):\n",
    "    index_no = X_train_dummies.columns.get_loc(missing_columns[i])\n",
    "    X_test_dummies.insert(loc = index_no, column = missing_columns[i], value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into training and testing dat\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_dummies, y_train_label, train_size = 0.7, random_state = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction: My prediction is that the Random Forest will be have a higher score compared to the Logistic Regression because the dataset is a mix of numbers and categorical entrees. I feel that a linear modeling of the categorical information will be less accurate. Another thing is that the averaging that is happening within the Random Forest Classifier adds value in model's prediction because it supported by the samples behind the average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.7158104621158808\n",
      "Testing Data Score: 0.5672054444917056\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   High Risk       0.61      0.36      0.46      2351\n",
      "    Low Risk       0.55      0.77      0.64      2351\n",
      "\n",
      "    accuracy                           0.57      4702\n",
      "   macro avg       0.58      0.57      0.55      4702\n",
      "weighted avg       0.58      0.57      0.55      4702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the unscaled data and print the model score\n",
    "# Create a logistic regression model\n",
    "lr_classifier = LogisticRegression(max_iter=12500)\n",
    "\n",
    "# Fit (train) model using the training data\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model by using the test data\n",
    "print(f\"Training Data Score: {lr_classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {lr_classifier.score(X_test_dummies, y_test_label)}\\n\")\n",
    "y_test_pred1 = lr_classifier.predict(X_test_dummies)\n",
    "print(classification_report(y_test_label, y_test_pred1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9998827117053718\n",
      "Testing Score: 0.6501488728200766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   High Risk       0.61      0.83      0.70      2351\n",
      "    Low Risk       0.73      0.47      0.57      2351\n",
      "\n",
      "    accuracy                           0.65      4702\n",
      "   macro avg       0.67      0.65      0.64      4702\n",
      "weighted avg       0.67      0.65      0.64      4702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "\n",
    "# Fit (train) model using the training data\n",
    "rfc_classifier = RandomForestClassifier(random_state=24, n_estimators=50).fit(X_train, y_train)\n",
    "\n",
    "# Validate the model by using the test data\n",
    "print(f'Training Score: {rfc_classifier.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {rfc_classifier.score(X_test_dummies, y_test_label)}\\n')\n",
    "y_test_pred2 = rfc_classifier.predict(X_test_dummies)\n",
    "print(classification_report(y_test_label, y_test_pred2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results: The Random Forest performed better than the Logistic Regression for both training and testing data scores. This is aligned with my initial prediction. Not scaling the data probably affected the Logical Regression making it less accurate. I still believe that the averaging of the results from the multiple trees make the Random Forest more robusts and have a higher score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_dummies_scaled = scaler.transform(X_test_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions: I think the Logistic Regression will benefit from the scaling process because it will make all the column data be in comparable values, and I think this help in better regression results. I still think that the Random Forest Classifier will produce better scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.7247243725076238\n",
      "Testing Data Score: 0.7662696724797958\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   High Risk       0.77      0.75      0.76      2351\n",
      "    Low Risk       0.76      0.78      0.77      2351\n",
      "\n",
      "    accuracy                           0.77      4702\n",
      "   macro avg       0.77      0.77      0.77      4702\n",
      "weighted avg       0.77      0.77      0.77      4702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the scaled data and print the model score\n",
    "# Create a log istic regression model\n",
    "scaled_lr_classifier = LogisticRegression(max_iter=12500)\n",
    "\n",
    "# Fit (train) model using the scaled training data\n",
    "scaled_lr_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Validate the model by using the scaled test data\n",
    "print(f\"Training Data Score: {scaled_lr_classifier.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {scaled_lr_classifier.score(X_test_dummies_scaled, y_test_label)}\\n\")\n",
    "y_test_pred3 = scaled_lr_classifier.predict(X_test_dummies_scaled)\n",
    "print(classification_report(y_test_label, y_test_pred3, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9998827117053718\n",
      "Testing Score: 0.65142492556359\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   High Risk       0.61      0.83      0.70      2351\n",
      "    Low Risk       0.74      0.47      0.58      2351\n",
      "\n",
      "    accuracy                           0.65      4702\n",
      "   macro avg       0.67      0.65      0.64      4702\n",
      "weighted avg       0.67      0.65      0.64      4702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model on the scaled data and print the model score\n",
    "# Fit (train) model using the scaled training data\n",
    "scaled_rfc_classifier = RandomForestClassifier(random_state=24, n_estimators=50).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Validate the model by using the scaled test data\n",
    "print(f'Training Score: {scaled_rfc_classifier.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {scaled_rfc_classifier.score(X_test_dummies_scaled, y_test_label)}')\n",
    "print()\n",
    "y_test_pred4 = scaled_rfc_classifier.predict(X_test_dummies_scaled)\n",
    "print(classification_report(y_test_label, y_test_pred4, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results: After scaling the data and training the models again, the Logistic Regression model scored higher when the 2020 data was used. The Random Forest Classifier still had a higher score when the training data is used for the scoring. The scaling benefited the Logistic Regression because it scaled all the data to a range of -2 to 2 thus removing any bias due to big values. The last result is opposite my initial prediction, it could be that Logistic Regression is better for certain analysis and Random Forest Classifier is better for others. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
